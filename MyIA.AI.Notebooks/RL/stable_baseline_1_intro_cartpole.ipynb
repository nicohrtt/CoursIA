{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyyN-2qyK_T2"
      },
      "source": [
        "# Tutoriel Stable Baselines3 - Premiers pas\n",
        "\n",
        "\n",
        "Stable-Baselines3 : https://github.com/DLR-RM/stable-baselines3\n",
        "\n",
        "Documentation : https://stable-baselines3.readthedocs.io/en/master/\n",
        "\n",
        "RL Baselines3 zoo : https://github.com/DLR-RM/rl-baselines3-zoo\n",
        "\n",
        "\n",
        "[RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) est un framework d’entraînement pour l’Apprentissage par Renforcement (RL), basé sur Stable Baselines3.\n",
        "\n",
        "Il fournit des scripts pour entraîner des agents, les évaluer, réaliser des recherches d’hyperparamètres, tracer des courbes de résultats et enregistrer des vidéos.\n",
        "\n",
        "Source:  https://github.com/araffin/rl-tutorial-jnrr19\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Dans ce notebook, vous allez apprendre les bases d’utilisation de la librairie Stable Baselines3 : comment créer un modèle RL, l’entraîner et l’évaluer. Comme toutes les algorithmes partagent la même interface, nous verrons qu’il est très simple de passer d’un algorithme à un autre.\n",
        "\n",
        "## Installer les dépendances et Stable Baselines3 avec Pip\n",
        "\n",
        "La liste complète des dépendances est disponible dans le [README](https://github.com/DLR-RM/stable-baselines3).\n",
        "\n",
        "Pour installer :\n",
        "```\n",
        "pip install stable-baselines3[extra]\n",
        "```\n",
        "\n",
        "---\n",
        "**Rappels sur l’Apprentissage par Renforcement (AR)**  \n",
        "- Un agent interagit avec un environnement au fil de pas de temps.  \n",
        "- À chaque pas, il reçoit un `state` (observation), choisit une `action` et obtient une `reward`.  \n",
        "- Le but est de maximiser la somme des récompenses.  \n",
        "- Stable-Baselines3 fournit un ensemble d’algorithmes pour entraîner cet agent sur divers environnements.\n",
        "\n",
        "*Astuce* : Pour ceux qui découvrent Gym, explorez rapidement `env.action_space` et `env.observation_space` pour comprendre les dimensions et les types d’actions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWskDE2c9WoN"
      },
      "outputs": [],
      "source": [
        "# Sous Windows, on ne fait pas d'apt-get.\n",
        "# %apt-get update && apt-get install ffmpeg freeglut3-dev xvfb  # Pour la visualisation sous Linux\n",
        "%pip install \"stable-baselines3[extra]>=2.0.0a4\" moviepy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U29X1-B-AIKE"
      },
      "outputs": [],
      "source": [
        "import stable_baselines3\n",
        "\n",
        "print(f\"{stable_baselines3.__version__=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtY8FhliLsGm"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcX8hEcaUpR0"
      },
      "source": [
        "Stable-Baselines3 fonctionne avec des environnements qui suivent l’interface [gym](https://stable-baselines.readthedocs.io/en/master/guide/custom_env.html).\n",
        "Vous pouvez trouver une liste d’environnements disponibles [ici](https://gym.openai.com/envs/#classic_control).\n",
        "\n",
        "Il est aussi recommandé de regarder le [code source](https://github.com/openai/gym) pour en savoir plus sur l’espace d’observation et d’action de chaque environnement, car gym ne fournit pas de documentation très détaillée.\n",
        "Tous les algorithmes ne sont pas compatibles avec tous les espaces d’action. Vous trouverez plus d’informations dans ce [tableau récapitulatif](https://stable-baselines.readthedocs.io/en/master/guide/algos.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIedd7Pz9sOs"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "print(f\"{gym.__version__=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ae32CtgzTG3R"
      },
      "source": [
        "La première chose dont vous avez besoin est d’importer la classe de l’algorithme de RL que vous souhaitez utiliser. Consultez la documentation pour savoir quel algorithme utiliser dans quel contexte.\n",
        "\n",
        "PPO est un algorithme on-policy, ce qui signifie que les données utilisées pour la mise à jour des réseaux proviennent de la politique courante. À l’inverse, un algo off-policy comme DQN peut réutiliser des données issues de politiques antérieures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7tKaBFrTR0a"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3 import PPO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0_8OQbOTTNT"
      },
      "source": [
        "Ensuite, vous pouvez importer la classe de politique (policy) qui servira à créer les réseaux (pour la fonction de politique et la fonction de valeur). Ce n’est pas obligatoire : vous pouvez directement utiliser des chaînes de caractères lors de la création du modèle, par exemple :\n",
        "```PPO('MlpPolicy', env)``` au lieu de ```PPO(MlpPolicy, env)```.\n",
        "\n",
        "Notez que certains algorithmes comme `SAC` ont leur propre `MlpPolicy`, donc l’utilisation de la chaîne de caractères est généralement recommandée."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROUJr675TT01"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.ppo import MlpPolicy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RapkYvTXL7Cd"
      },
      "source": [
        "## Créer l’environnement Gym et instancier l’agent\n",
        "\n",
        "Dans cet exemple, nous allons utiliser l’environnement CartPole, un problème classique de contrôle.\n",
        "\n",
        "« Un poteau est attaché par un joint non-actionné à un chariot, qui se déplace le long d’un rail sans frottement. Le système est contrôlé en appliquant une force de +1 ou -1 sur le chariot. Le pendule commence à la verticale, et l’objectif est de l’empêcher de tomber. Une récompense de +1 est accordée à chaque pas de temps pendant lequel le poteau reste en position verticale. »\n",
        "\n",
        "Environnement CartPole : [https://gymnasium.farama.org/environments/classic_control/cart_pole/](https://gymnasium.farama.org/environments/classic_control/cart_pole/)\n",
        "\n",
        "![Cartpole](https://cdn-images-1.medium.com/max/1143/1*h4WTQNVIsvMXJTCpXm_TAw.gif)\n",
        "\n",
        "Les environnements vectorisés (vecenv) permettent de faciliter l’entraînement en parallèle. Ici, nous utilisons un seul processus, donc `DummyVecEnv`.\n",
        "\n",
        "Nous choisissons `MlpPolicy` car l’entrée de CartPole est un vecteur de caractéristiques (et non une image).\n",
        "\n",
        "Le type d’action (discrète/continue) sera automatiquement déduit de l’espace d’action de l’environnement.\n",
        "\n",
        "Ici, nous utilisons [Proximal Policy Optimization](https://stable-baselines.readthedocs.io/en/master/modules/ppo2.html), qui est une méthode Actor-Critic : elle utilise une fonction de valeur pour améliorer la descente de gradient de la politique (en réduisant la variance).\n",
        "\n",
        "PPO combine des idées d’[A2C](https://stable-baselines.readthedocs.io/en/master/modules/a2c.html) (plusieurs workers et bonus d’entropie pour encourager l’exploration) et de [TRPO](https://stable-baselines.readthedocs.io/en/master/modules/trpo.html) (utilisation d’une région de confiance pour stabiliser l’apprentissage et éviter des chutes drastiques de performance).\n",
        "\n",
        "PPO est un algorithme on-policy : les trajectoires utilisées pour mettre à jour les réseaux doivent être collectées avec la politique la plus récente.\n",
        "Il est généralement moins échantillonnement-efficace que des algorithmes off-policy comme [DQN](https://stable-baselines.readthedocs.io/en/master/modules/dqn.html), [SAC](https://stable-baselines.readthedocs.io/en/master/modules/sac.html) ou [TD3](https://stable-baselines.readthedocs.io/en/master/modules/td3.html), mais il est souvent plus rapide en temps d’horloge réel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUWGZp3i9wyf"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "model = PPO(MlpPolicy, env, verbose=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4efFdrQ7MBvl"
      },
      "source": [
        "Nous créons une fonction utilitaire pour évaluer l’agent :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63M8mSKR-6Zt"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, num_episodes=100, deterministic=True):\n",
        "    \"\"\"\n",
        "    Évalue un agent RL.\n",
        "    :param model: (BaseRLModel) l’agent RL\n",
        "    :param num_episodes: (int) nombre d’épisodes sur lesquels évaluer\n",
        "    :param deterministic: (bool) si on utilise une politique déterministe\n",
        "    :return: (float) Récompense moyenne sur les num_episodes derniers épisodes\n",
        "    \"\"\"\n",
        "    # Cette fonction ne fonctionne que pour un seul environnement\n",
        "    vec_env = model.get_env()\n",
        "    all_episode_rewards = []\n",
        "    for i in range(num_episodes):\n",
        "        episode_rewards = []\n",
        "        done = False\n",
        "        obs = vec_env.reset()\n",
        "        while not done:\n",
        "            # _states n’est utile que lorsque l’on utilise des politiques LSTM\n",
        "            action, _states = model.predict(obs, deterministic=deterministic)\n",
        "            # Ici, action, rewards et dones sont des tableaux\n",
        "            # car nous utilisons un environnement vectorisé\n",
        "            obs, reward, done, info = vec_env.step(action)\n",
        "            episode_rewards.append(reward)\n",
        "\n",
        "        all_episode_rewards.append(sum(episode_rewards))\n",
        "\n",
        "    mean_episode_reward = np.mean(all_episode_rewards)\n",
        "    print(\"Récompense moyenne :\", mean_episode_reward, \"Nombre d’épisodes :\", num_episodes)\n",
        "\n",
        "    return mean_episode_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hkyafs--gJz"
      },
      "source": [
        "En fait, Stable-Baselines3 fournit déjà un utilitaire similaire :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6ZNldIR-fce"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.evaluation import evaluate_policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjEVOIY8NVeK"
      },
      "source": [
        "Évaluons l’agent non entraîné ; il devrait agir de façon essentiellement aléatoire."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDHLMA6NFk95"
      },
      "outputs": [],
      "source": [
        "# Nous utilisons un environnement distinct pour l’évaluation\n",
        "eval_env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "\n",
        "# Agent aléatoire, avant entraînement\n",
        "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=100)\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5UoXTZPNdFE"
      },
      "source": [
        "## Entraîner l’agent et l’évaluer\n",
        "\n",
        "**Hyperparamètres clés**  \n",
        "- `total_timesteps`: nombre total de pas d’entraînement (interactions avec l’environnement).  \n",
        "- `learning_rate`: définit la vitesse à laquelle les poids sont mis à jour.  \n",
        "- `n_steps` (ou équivalent): longueur des trajectoires collectées avant chaque mise à jour, etc.  \n",
        "- `batch_size`: taille de l’échantillon pour chaque itération d’apprentissage.  \n",
        "\n",
        "*Tip* : N’hésitez pas à ajuster progressivement `total_timesteps` si la convergence n’est pas satisfaisante.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4cfSXIB-pTF"
      },
      "outputs": [],
      "source": [
        "# Entraînons l’agent pendant 10 000 pas\n",
        "model.learn(total_timesteps=10_000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygl_gVmV_QP7"
      },
      "outputs": [],
      "source": [
        "# Évaluons l’agent entraîné\n",
        "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=100)\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A00W6yY3NkHG"
      },
      "source": [
        "Visiblement, l’entraînement s’est bien déroulé : la récompense moyenne a beaucoup augmenté !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVm9QPNVwKXN"
      },
      "source": [
        "### Préparer l’enregistrement vidéo\n",
        "\n",
        "**Note sur la visualisation**  \n",
        "- Sous Windows, on n’a pas besoin de créer de display virtuel (`xvfb`).  \n",
        "- Sur Linux, si vous n’avez pas d’interface graphique, vous devrez lancer un display virtuel pour capturer des frames (`xvfb-run`).  \n",
        "- Les fonctions ci-dessous utilisent `render_mode=\\\"rgb_array\\\"` pour récupérer les images directement.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPyfQxD5z26J"
      },
      "outputs": [],
      "source": [
        "# Sous Windows, pas besoin de lancer un display virtuel.\n",
        "# On commente donc la partie suivante (utile surtout sous Linux) :\n",
        "# import os\n",
        "# os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "# os.environ['DISPLAY'] = ':1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLzXxO8VMD6N"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "from pathlib import Path\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "def show_videos(video_path=\"\", prefix=\"\"):\n",
        "    \"\"\"\n",
        "    Inspiré de : https://github.com/eleurent/highway-env\n",
        "\n",
        "    :param video_path: (str) chemin vers le dossier contenant les vidéos\n",
        "    :param prefix: (str) filtre sur le préfixe des noms de fichiers vidéo\n",
        "    \"\"\"\n",
        "    html = []\n",
        "    for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
        "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "        html.append(\n",
        "            \"\"\"<video alt=\"{}\" autoplay\n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>\"\"\".format(\n",
        "                mp4, video_b64.decode(\"ascii\")\n",
        "            )\n",
        "        )\n",
        "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTRNUfulOGaF"
      },
      "source": [
        "Nous allons enregistrer une vidéo à l’aide de [VecVideoRecorder](https://stable-baselines.readthedocs.io/en/master/guide/vec_envs.html#vecvideorecorder). Vous en apprendrez davantage sur ces wrappers dans le prochain notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Trag9dQpOIhx"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "\n",
        "def record_video(env_id, model, video_length=500, prefix=\"\", video_folder=\"videos/\"):\n",
        "    \"\"\"\n",
        "    :param env_id: (str)\n",
        "    :param model: (RL model)\n",
        "    :param video_length: (int)\n",
        "    :param prefix: (str)\n",
        "    :param video_folder: (str)\n",
        "    \"\"\"\n",
        "    eval_env = DummyVecEnv([lambda: gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")])\n",
        "    # Commencer la vidéo au pas=0 et enregistrer 500 étapes\n",
        "    eval_env = VecVideoRecorder(\n",
        "        eval_env,\n",
        "        video_folder=video_folder,\n",
        "        record_video_trigger=lambda step: step == 0,\n",
        "        video_length=video_length,\n",
        "        name_prefix=prefix,\n",
        "    )\n",
        "\n",
        "    obs = eval_env.reset()\n",
        "    for _ in range(video_length):\n",
        "        action, _ = model.predict(obs)\n",
        "        obs, _, _, _ = eval_env.step(action)\n",
        "\n",
        "    # Fermer le recorder vidéo\n",
        "    eval_env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOObbeu5MMlR"
      },
      "source": [
        "### Visualiser l’agent entraîné\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iATu7AiyMQW2"
      },
      "outputs": [],
      "source": [
        "record_video(\"CartPole-v1\", model, video_length=500, prefix=\"ppo-cartpole\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-n4i-fW3NojZ"
      },
      "outputs": [],
      "source": [
        "show_videos(\"videos\", prefix=\"ppo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y8zg4V566qD"
      },
      "source": [
        "## Bonus : entraîner un modèle RL en une seule ligne\n",
        "\n",
        "La classe de politique utilisée sera déduite automatiquement et l’environnement sera créé automatiquement également. Cela fonctionne parce que les deux sont [enregistrés](https://stable-baselines.readthedocs.io/en/master/guide/quickstart.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaOPfOrwWEP4"
      },
      "outputs": [],
      "source": [
        "model = PPO('MlpPolicy', \"CartPole-v1\", verbose=1).learn(1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrI6f5fWnzp-"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Dans ce notebook, nous avons vu :\n",
        "- comment définir et entraîner un modèle RL à l’aide de Stable Baselines3 (en une seule ligne de code) ;)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "1_getting_started_FR.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
