{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook : G√©n√©ration d'Images, Low-Code AI, Function Calling, RAG\n",
    "\n",
    "Dans ce notebook, nous allons tour √† tour d√©couvrir :\n",
    "\n",
    "1. Comment g√©n√©rer des **images** √† partir de prompts en texte (ex: DALL-E, Midjourney).\n",
    "2. Comment cr√©er des **applications low-code** enrichies par l'IA, gr√¢ce √† **Power Platform** (Copilot, AI Builder).\n",
    "3. **Function Calling** c√¥t√© OpenAI : structurer les r√©ponses d‚Äôun LLM pour d√©clencher des actions.\n",
    "4. **RAG** (Retrieval Augmented Generation) et **bases vectorielles** (indexation, recherche s√©mantique, chunking, etc.).\n",
    "\n",
    "\n",
    "\n",
    "## Pr√©requis & Installation\n",
    "\n",
    "- **Python 3.9+** (ou version ult√©rieure).\n",
    "- Un compte [OpenAI](https://platform.openai.com/) et une cl√© d‚ÄôAPI valide.\n",
    "- Le fichier `.env` contenant votre cl√© d‚ÄôAPI :\n",
    "OPENAI_API_KEY=sk-...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pourquoi la g√©n√©ration d'images via IA ?\n",
    "\n",
    "Les mod√®les de g√©n√©ration d'images, tels que **DALL-E** (OpenAI) ou **Midjourney**, ont la capacit√© de cr√©er des visuels originaux √† partir de simples descriptions textuelles (prompts).  \n",
    "- **Applications concr√®tes** :  \n",
    "  - Design rapide de prototypes (marketing, publicit√©)  \n",
    "  - Cr√©ation artistique (concept art, storyboards)  \n",
    "  - Illustrations p√©dagogiques ou infographiques  \n",
    "- **Limitations** :  \n",
    "  - Les images peuvent contenir des incoh√©rences (proportions bizarres, doigts suppl√©mentaires, etc.)  \n",
    "  - Certaines requ√™tes contraires aux politiques d‚Äôutilisation peuvent √™tre bloqu√©es  \n",
    "\n",
    "Dans la suite, nous allons voir comment **OpenAI** g√®re la g√©n√©ration d‚Äôimages via l‚ÄôAPI `images.generate()`, et comment int√©grer ces visuels dans un flux d'automatisation (Low-Code) ou dans des applications web.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Installation\n",
    "# ============================\n",
    "\n",
    "%pip install openai tiktoken python-dotenv\n",
    "# Remarque : Aucune fin de ligne en commentaire pour √©viter l'erreur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "# from PIL import Image\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Charger le fichier .env pour la cl√© OPENAI_API_KEY\n",
    "load_dotenv()\n",
    "\n",
    "# Config globale\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "try:\n",
    "    response = openai.images.generate(\n",
    "        prompt=\"Lapin sur un cheval tenant une sucette, dans un champ brumeux\"\n",
    "    )\n",
    "    # response est un ImagesResponse\n",
    "    image_url = response.data[0].url\n",
    "    print(\"Image URL:\", image_url)\n",
    "\n",
    "    # T√©l√©chargement de l'image\n",
    "    # img_data = requests.get(image_url).content\n",
    "    # with open(\"my_image.png\",\"wb\") as f:\n",
    "    #     f.write(img_data)\n",
    "\n",
    "    # # Ouverture\n",
    "    # img = Image.open(\"my_image.png\")\n",
    "    # img.show()\n",
    "    \n",
    "    \n",
    "    image = Image(url= image_url)\n",
    "    display(image)\n",
    "\n",
    "except openai.APIConnectionError as e:\n",
    "    print(\"Erreur de connexion r√©seau:\", e)\n",
    "except openai.RateLimitError as e:\n",
    "    print(\"Limite atteinte ou quota d√©pass√©:\", e)\n",
    "except openai.APIStatusError as e:\n",
    "    print(\"Erreur HTTP renvoy√©e par l'API (4xx, 5xx, etc.):\", e)\n",
    "except openai.APIError as e:\n",
    "    print(\"Autre erreur OpenAI:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "#  Comparaison de prompts d'images\n",
    "# ============================\n",
    "\n",
    "image_prompts = [\n",
    "    \"A small kitten wearing a hat, cartoon style\",\n",
    "    \"A realistic portrait of a small kitten wearing a cowboy hat in the desert\",\n",
    "    \"A small kitten wearing a futuristic helmet in cyberpunk style, neon colors\"\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(image_prompts):\n",
    "    try:\n",
    "        print(f\"--- Prompt #{i+1}: {prompt} ---\")\n",
    "        response_img = openai.images.generate(prompt=prompt)\n",
    "        img_url = response_img.data[0].url\n",
    "        print(\"Image URL:\", img_url)\n",
    "        # Optionnel : display() si tu es dans un environnement Jupyter\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Erreur lors de la g√©n√©ration d'image:\", e)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 M√©ta-prompts et usage responsable\n",
    "\n",
    "Pour g√©rer un usage plus responsable et filtrer des images non souhait√©es, on peut ajouter un \n",
    "**meta-prompt** en amont, d√©crivant les restrictions (ex: Safe for Work, No adult content, etc.).\n",
    "\n",
    "Ex:\n",
    "You are an assistant that only generates children-friendly images. [... consignes ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Low-Code AI Apps (Power Platform)\n",
    "\n",
    "## 2.1 Introduction\n",
    "Power Platform inclut :\n",
    "- Power Apps (construction rapide d'apps)\n",
    "- Power Automate (workflows et automatisations)\n",
    "- Dataverse (stockage de donn√©es)\n",
    "- AI Builder (mod√®les IA pr√©-construits)\n",
    "- Copilot (assistant pour g√©n√©rer tables, flux, e-mails)\n",
    "\n",
    "Avantages : construction **no-code / low-code** pour mettre en place des solutions rapidement, \n",
    "y compris connect√©es √† des services IA.\n",
    "\n",
    "\n",
    "## 2.2 Copilot dans Power Apps : Student Assignment Tracker\n",
    "\n",
    "Exemple : On veut un **Student Assignment Tracker**.\n",
    "\n",
    "1. Sur la home de [Power Apps](https://make.powerapps.com), on saisit dans la zone Copilot : \n",
    "   \"I want an app to track and manage student assignments.\"\n",
    "2. Copilot propose une table Dataverse (champs Title, DateDue, StudentName, etc.)\n",
    "3. Personnaliser la table (ajouter `StudentEmail`, etc.)\n",
    "4. Cliquer \"Create app\" => Copilot g√©n√®re une **Canvas App** auto.\n",
    "5. Ajouter une page (screen) pour \"Envoyer un email\" (Prompt : \"I want a screen to send an email to the student\").\n",
    "\n",
    "On obtient en quelques clics un d√©but d'application.\n",
    "\n",
    "\n",
    "## 2.3 Copilot dans Power Automate : Invoice Processing\n",
    "\n",
    "M√™me concept : Dans [Power Automate](https://make.powerautomate.com),\n",
    "on demande \"Process an invoice when it arrives in my mailbox\", \n",
    "Copilot propose un flux (trigger: new mail arrives + extractions + email)...\n",
    "\n",
    "On peut ensuite y int√©grer **AI Builder** : \n",
    "- ex: le pr√©built model \"Invoice Processing\" pour extraire `supplier`, `amount`, etc.\n",
    "- stocker dans Dataverse, \n",
    "- email de confirmation.\n",
    "\n",
    "C‚Äôest un gros gain de temps pour la finance ou la logistique !\n",
    "\n",
    "\n",
    "# 3. Function Calling (OpenAI)\n",
    "\n",
    "\n",
    "## 3.1 Pourquoi ?\n",
    "\n",
    "Sans function calling, le LLM renvoie du texte non structur√©. \n",
    "Difficile d‚Äôautomatiser (ex: parse JSON, ex√©cuter une fonction tierce).\n",
    "Avec function calling, on d√©clare un `schema` JSON, \n",
    "le LLM r√©pond par un `function_call`: \n",
    "- Nom de la fonction \n",
    "- Arguments structur√©s\n",
    "\n",
    "Ensuite on ex√©cute la fonction en Python (ou autre).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Find me a good course for a beginner developer to learn Azure.\"}\n",
    "]\n",
    "\n",
    "functions = [\n",
    "  {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"description\": \"Retrieves relevant courses based on role, product & level\",\n",
    "    \"parameters\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"role\":   {\"type\":\"string\",\"description\":\"the role of the user\"},\n",
    "        \"product\":{\"type\":\"string\",\"description\":\"the product/tech\"},\n",
    "        \"level\": {\"type\":\"string\",\"description\":\"the user skill level\"}\n",
    "      },\n",
    "      \"required\": [\"role\",\"product\",\"level\"]\n",
    "    }\n",
    "  }\n",
    "]\n",
    "\n",
    "\n",
    "try:\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  \n",
    "        messages=messages,\n",
    "        functions=functions,\n",
    "        function_call=\"auto\"  # laisse le LLM d√©cider s‚Äôil appelle la fonction\n",
    "    )\n",
    "\n",
    "    print(\"R√©ponse brute:\\n\", response.choices[0].message)\n",
    "\n",
    "except openai.RateLimitError as e:\n",
    "    print(\"Limite atteinte:\", e)\n",
    "except openai.APIError as e:\n",
    "    print(\"Autre erreur:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def search_courses(role,product,level):\n",
    "    # Ton code Python => Appel API Microsoft Learn\n",
    "    # On renvoie un JSON/string\n",
    "    return \"Liste de cours: Azure Fundamentals, etc.\"\n",
    "\n",
    "resp_msg = response.choices[0].message\n",
    "if resp_msg.function_call:\n",
    "    fn_name = resp_msg.function_call.name\n",
    "    fn_args = json.loads(resp_msg.function_call.arguments)\n",
    "    \n",
    "    # Ex√©cuter la fonction Python correspondante\n",
    "    result = search_courses(**fn_args)\n",
    "\n",
    "    # On cr√©e deux messages :\n",
    "    # 1) le function_call\n",
    "    # 2) le role=\"function\" + content du r√©sultat\n",
    "    second_messages = [\n",
    "      {\"role\":\"assistant\",\"function_call\": {\"name\":fn_name,\"arguments\":resp_msg.function_call.arguments}},\n",
    "      {\"role\":\"function\",\"name\":fn_name,\"content\": result}\n",
    "    ]\n",
    "\n",
    "    # On relance le chat\n",
    "    final_resp = openai.chat.completions.create(\n",
    "       model=\"gpt-4o-mini\",\n",
    "       messages=messages + second_messages\n",
    "    )\n",
    "    print(\"R√©ponse finale:\\n\", final_resp.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Calling Avanc√© avec OpenAI : Cha√Ænage de Fonctions  \n",
    "\n",
    "Ce bloc de code illustre un exemple avanc√© de **Function Calling** avec OpenAI, permettant √† l'IA d'orchestrer plusieurs appels de fonctions de mani√®re autonome.  \n",
    "\n",
    "### üõ† Fonctionnalit√©s mises en ≈ìuvre :  \n",
    "1. **Premier appel √† l'API OpenAI** :  \n",
    "   - L'utilisateur demande de planifier une r√©union.  \n",
    "   - L'IA d√©tecte qu'il faut appeler `create_meeting_event` et g√©n√®re les arguments n√©cessaires (`topic`, `date`, `participants`).  \n",
    "   - Le mod√®le ne renvoie pas de texte brut, mais un `function_call` contenant les param√®tres de la r√©union.  \n",
    "\n",
    "2. **Ex√©cution locale de `create_meeting_event` en Python** :  \n",
    "   - La fonction g√©n√®re un objet √©v√©nement fictif avec un `event_id`.  \n",
    "   - Ce r√©sultat est transmis √† l'IA en tant que r√©ponse fonctionnelle (`role=\"function\"`).  \n",
    "\n",
    "3. **Deuxi√®me appel √† l'API OpenAI** :  \n",
    "   - Sur la base de l'√©v√©nement cr√©√©, l'IA d√©cide de g√©n√©rer un email de confirmation en appelant `send_email`.  \n",
    "   - L'IA fournit les arguments (`subject`, `body`, `recipients`).  \n",
    "\n",
    "4. **Ex√©cution locale de `send_email` en Python** :  \n",
    "   - Simulation de l'envoi d'email avec un affichage console.  \n",
    "   - Message de confirmation `\"Email sent successfully!\"`.  \n",
    "\n",
    "### üìå Preuve du bon fonctionnement :  \n",
    "‚úÖ **Cha√Ænage r√©ussi** : OpenAI a d√©clench√© **deux appels de fonction distincts**, prouvant la capacit√© du mod√®le √† raisonner sur plusieurs √©tapes.  \n",
    "‚úÖ **Ex√©cution hybride** : L'IA d√©cide des actions √† effectuer, mais l'ex√©cution est d√©l√©gu√©e au code Python.  \n",
    "‚úÖ **Application possible** : Ce workflow peut √™tre adapt√© pour int√©grer des bases de donn√©es, envoyer de vrais emails ou automatiser des t√¢ches complexes.  \n",
    "\n",
    "üîπ **Exemple d'affichage console :**  \n",
    "```\n",
    "Raw response: ChatCompletionMessage(..., function_call=FunctionCall(...))\n",
    "=== Simulated Email ===\n",
    "Subject: Confirmation de la r√©union sur l'√©tat du projet\n",
    "To: ['Alice', 'Bob']\n",
    "Body: Bonjour Alice et Bob,\n",
    "Je vous confirme que la r√©union sur l'√©tat du projet est planifi√©e pour mardi prochain, le 31 octobre 2023, √† 10h.\n",
    "√Ä bient√¥t,\n",
    "L'√©quipe de gestion de projet\n",
    "Email function result: Email sent successfully!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Function Calling (avanc√©) avec 2 fonctions\n",
    "# ============================\n",
    "\n",
    "import json\n",
    "import openai\n",
    "\n",
    "def create_meeting_event(topic, date, participants):\n",
    "    # Exemple simul√© : cr√©ation d'un objet event\n",
    "    return {\n",
    "        \"event_id\": \"evt-001\",\n",
    "        \"topic\": topic,\n",
    "        \"date\": date,\n",
    "        \"participants\": participants\n",
    "    }\n",
    "\n",
    "def send_email(subject, body, recipients):\n",
    "    # Exemple simul√© : envoi d'un email\n",
    "    print(\"=== Simulated Email ===\")\n",
    "    print(\"Subject:\", subject)\n",
    "    print(\"To:\", recipients)\n",
    "    print(\"Body:\", body)\n",
    "    return \"Email sent successfully!\"\n",
    "\n",
    "functions = [\n",
    "    {\n",
    "        \"name\": \"create_meeting_event\",\n",
    "        \"description\": \"Cr√©er un √©v√©nement de r√©union\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"topic\": {\"type\": \"string\"},\n",
    "                \"date\": {\"type\": \"string\"},\n",
    "                \"participants\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\"type\": \"string\"}\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"topic\", \"date\", \"participants\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"send_email\",\n",
    "        \"description\": \"Envoyer un email\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"subject\": {\"type\": \"string\"},\n",
    "                \"body\": {\"type\": \"string\"},\n",
    "                \"recipients\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\"type\": \"string\"}\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"subject\", \"body\", \"recipients\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "messages_fc = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"Planifie une r√©union sur l'√©tat du projet pour mardi prochain √† 10h \"\n",
    "            \"avec Alice et Bob, puis envoie un email de confirmation.\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "# L'appel initial : le mod√®le peut d√©cider d'appeler\n",
    "# create_meeting_event, send_email, ou rien (function_call=\"auto\").\n",
    "response_fc = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages_fc,\n",
    "    functions=functions,\n",
    "    function_call=\"auto\"\n",
    ")\n",
    "\n",
    "assistant_msg = response_fc.choices[0].message\n",
    "print(\"Raw response:\", assistant_msg)\n",
    "\n",
    "# V√©rifier si le mod√®le appelle une fonction\n",
    "if assistant_msg.function_call:\n",
    "    fn_name = assistant_msg.function_call.name\n",
    "    fn_args_str = assistant_msg.function_call.arguments  # cha√Æne JSON\n",
    "    fn_args = json.loads(fn_args_str)\n",
    "\n",
    "    if fn_name == \"create_meeting_event\":\n",
    "        # 1) Ex√©cuter la fonction create_meeting_event c√¥t√© Python\n",
    "        result_event = create_meeting_event(**fn_args)\n",
    "\n",
    "        # 2) Cr√©er de nouveaux messages (assistant + function)\n",
    "        second_messages = messages_fc + [\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"function_call\": {\n",
    "                    \"name\": fn_name,\n",
    "                    \"arguments\": json.dumps(fn_args)\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"function\",\n",
    "                \"name\": fn_name,\n",
    "                \"content\": json.dumps(result_event)\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # 3) Relancer le chat pour voir si le mod√®le appelle la 2e fonction\n",
    "        second_response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=second_messages,\n",
    "            functions=functions,\n",
    "            function_call=\"auto\"\n",
    "        )\n",
    "        second_msg = second_response.choices[0].message\n",
    "\n",
    "        if second_msg.function_call:\n",
    "            fn2_name = second_msg.function_call.name\n",
    "            fn2_args_str = second_msg.function_call.arguments\n",
    "            fn2_args = json.loads(fn2_args_str)\n",
    "\n",
    "            if fn2_name == \"send_email\":\n",
    "                # Ex√©cution de la seconde fonction\n",
    "                result_email = send_email(**fn2_args)\n",
    "                print(\"Email function result:\", result_email)\n",
    "            else:\n",
    "                print(f\"The model called a different function: {fn2_name}\")\n",
    "        else:\n",
    "            print(\"No second function call was triggered.\")\n",
    "    else:\n",
    "        print(f\"The model called a different function: {fn_name}\")\n",
    "else:\n",
    "    print(\"No function call triggered by the assistant.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Retrieval Augmented Generation & Vector Databases\n",
    "\n",
    "## 4.1 Principe\n",
    "Un LLM (ex: GPT) a une limite : il ne conna√Æt pas forc√©ment nos documents internes. \n",
    "RAG => on stocke nos docs dans une base vectorielle (embeddings), \n",
    "puis √† chaque question, on envoie au LLM les passages pertinents (retrieval + augmentation).\n",
    "\n",
    "## 4.2 Cr√©ation d‚Äôune base vectorielle\n",
    "\n",
    "- On d√©coupe (chunk) nos documents en petits segments (ex: 400 tokens).\n",
    "- On calcule embeddings (ex: text-embedding-ada-002).\n",
    "- On stocke : ex. Cosmos DB, Pinecone, ChromaDB, Elasticsearch, Qdrant, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn numpy pandas requests beautifulsoup4 lxml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL du d√©bat\n",
    "url = \"https://home.nps.gov/liho/learn/historyculture/debate1.htm\"\n",
    "\n",
    "# Requ√™te HTTP\n",
    "response = requests.get(url)\n",
    "html = response.text  # contenu HTML sous forme de string\n",
    "\n",
    "# print(html)\n",
    "\n",
    "# On parse avec BeautifulSoup\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# S√©lection du noeud principal.\n",
    "# Selon ton info: div.ColumnMain:nth-child(2)\n",
    "# (Le \"nth-child(2)\" est parfois incertain, on peut tenter un select plus large.)\n",
    "main_div = soup.select_one(\"div.ColumnMain\")\n",
    "\n",
    "if not main_div:\n",
    "    raise ValueError(\"Impossible de trouver le div.ColumnMain dans la page !\")\n",
    "\n",
    "# Extraction du texte brut (on s√©pare par \" \" pour √©viter les collisions)\n",
    "debate_text = main_div.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "#Enregistrement du texte dans un fichier\n",
    "with open(\"debate.txt\", \"w\") as f:\n",
    "    f.write(debate_text)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement depuis le fichier\n",
    "with open(\"debate.txt\", \"r\") as f:\n",
    "    debate_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour d√©bogage:\n",
    "print(\"=== Longueur du texte r√©cup√©r√©:\", len(debate_text))\n",
    "print(debate_text[:10000], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"title\":\"First Debate: Ottawa, Illinois (NPS)\",\n",
    "            \"text\": debate_text\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_chunks(text, chunk_size=500, overlap=50):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = start + chunk_size\n",
    "        chunk_words = words[start:end]\n",
    "        chunk = \" \".join(chunk_words)\n",
    "        chunks.append(chunk)\n",
    "        start += (chunk_size - overlap)\n",
    "    return chunks\n",
    "\n",
    "rows = []\n",
    "for _, row in df.iterrows():\n",
    "    splitted = split_text_into_chunks(row[\"text\"], chunk_size=400, overlap=50)\n",
    "    for chunk in splitted:\n",
    "        rows.append({\n",
    "            \"title\": row[\"title\"],\n",
    "            \"chunk\": chunk\n",
    "        })\n",
    "\n",
    "df_chunks = pd.DataFrame(rows)\n",
    "print(\"Nombre de chunks =\", len(df_chunks))\n",
    "df_chunks.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialiser le client OpenAI (indispensable avec la nouvelle API)\n",
    "client = OpenAI()\n",
    "\n",
    "def create_embedding(text: str):\n",
    "    try:\n",
    "        response = client.embeddings.create(\n",
    "            model=\"text-embedding-3-large\",\n",
    "            input=[text]  # ‚ö†Ô∏è Doit √™tre une **liste**\n",
    "        )\n",
    "        return response.data[0].embedding  # Extraction correcte\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur lors de la g√©n√©ration d'embedding: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# G√©n√©rer les embeddings pour les chunks du DataFrame\n",
    "df_chunks[\"embedding\"] = df_chunks[\"chunk\"].apply(create_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Nettoyage des embeddings\n",
    "all_vectors = np.array([emb for emb in df_chunks[\"embedding\"] if emb is not None])  # Exclure None\n",
    "nn = NearestNeighbors(n_neighbors=3, metric=\"euclidean\")\n",
    "nn.fit(all_vectors)\n",
    "\n",
    "def retrieve(user_query: str) -> str:\n",
    "    try:\n",
    "        # G√©n√©rer l'embedding de la requ√™te\n",
    "        q_emb = create_embedding(user_query)\n",
    "        if q_emb is None:\n",
    "            return \"‚ö†Ô∏è Impossible de g√©n√©rer un embedding pour la requ√™te.\"\n",
    "\n",
    "        dist, idx = nn.kneighbors([q_emb])\n",
    "\n",
    "        # R√©cup√©rer les meilleurs chunks\n",
    "        best_chunks = df_chunks.iloc[idx[0]][\"chunk\"].tolist()\n",
    "        prompt = user_query + \"\\n\\n\" + \"\\n\".join(best_chunks)\n",
    "        \n",
    "        print(\"üîç Prompt augment√© g√©n√©r√©e :\\n\", prompt)\n",
    "\n",
    "        # ‚úÖ Appel OpenAI corrig√©\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content if response.choices else \"‚ö†Ô∏è Aucune r√©ponse g√©n√©r√©e.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"‚ö†Ô∏è Erreur lors de la r√©cup√©ration : {str(e)}\"\n",
    "\n",
    "\n",
    "# üî• Test avec la question sur Lincoln\n",
    "question = \"What did Lincoln argue about slavery in that first debate?\"\n",
    "answer = retrieve(question)\n",
    "print(\"üîç R√©ponse g√©n√©r√©e :\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonnes pratiques pour la RAG (Retrieval Augmented Generation)\n",
    "\n",
    "1. **Chunking**  \n",
    "   - D√©couper vos documents en segments de taille raisonnable (ex. 300-500 tokens), afin de mieux cibler les passages pertinents.\n",
    "2. **Indexation des embeddings**  \n",
    "   - Stocker les vecteurs dans une base adapt√©e (ex. Pinecone, Chroma, Elasticsearch vectoriel, Qdrant, ou Cosmos DB vector).\n",
    "3. **Filtrage et post-traitement**  \n",
    "   - Apr√®s avoir r√©cup√©r√© les chunks les plus proches s√©mantiquement, il peut √™tre utile de v√©rifier l‚Äôexactitude ou la coh√©rence des informations extraites.\n",
    "4. **R√©-int√©gration**  \n",
    "   - Ins√©rer les passages s√©lectionn√©s dans le prompt (par ex. ‚ÄúVoici un extrait : ...\\n\\n Maintenant, r√©ponds √† la question...‚Äù).  \n",
    "   - Ou bien utiliser un outil style [LangChain](https://github.com/hwchase17/langchain) qui facilite ce pipeline.\n",
    "5. **√âviter les hallucinations**  \n",
    "   - Demander explicitement au mod√®le de s‚Äôen tenir aux informations fournies dans les chunks.  \n",
    "   - En cas d‚Äôinsuffisance de donn√©es, demander au mod√®le de r√©pondre ‚ÄúJe ne sais pas‚Äù plut√¥t que d‚Äôinventer.\n",
    "\n",
    "L‚Äôobjectif est de combiner la **puissance du LLM** et l‚Äô**exactitude** de donn√©es externes (base documentaire, articles, PDF, etc.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conclusion & Pistes\n",
    "\n",
    "Nous avons explor√© :\n",
    "- la g√©n√©ration d‚Äôimages (DALL-E, prompts, meta-prompts),\n",
    "- la cr√©ation d‚Äôapps low-code Power Apps / Automate,\n",
    "- l‚Äôusage de Copilot & AI Builder pour des sc√©narios m√©tiers (tracking, invoice),\n",
    "- la structuration des r√©ponses via Function Calling,\n",
    "- RAG : indexer nos docs dans une base vectorielle et enrichir un LLM.\n",
    "\n",
    "Pistes d‚Äôexercices :\n",
    "- Am√©liorer les prompts d‚Äôimages (temp√©rature, variations, mask, etc.)\n",
    "- Cr√©er un flux complet dans Power Automate avec AI Builder\n",
    "- Mettre en place Function Calling plus complexe (multi-fonctions, error-handling)\n",
    "- Stocker un doc plus large (ex: 10 pages PDF) en chunks + RAG\n",
    "\n",
    "Fin de la synth√®se ! \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
