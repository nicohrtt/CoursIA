{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction à l'IA générative avec l'API OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan global du Notebook\n",
    "\n",
    "1. **Introduction Générale**  \n",
    "   - Contexte et définitions (IA Générative, LLMs, prompts, etc.)\n",
    "   - Présentation succincte des enjeux éthiques et de responsabilité\n",
    "\n",
    "2. **Premier Exemple de Code : Vérification de l'Environnement**  \n",
    "   - Installation et importation des bibliothèques (OpenAI ou Azure OpenAI)\n",
    "   - Test d’un prompt de base\n",
    "\n",
    "3. **Notions de Base sur les Prompts**  \n",
    "   - Tokens, embeddings, bases de la génération\n",
    "   - Exercices pratiques : tokenisation et génération\n",
    "\n",
    "4. **Exemple : Fabrications (Hallucinations) et Fiabilité**  \n",
    "   - Démonstration via un prompt volontairement ambigu\n",
    "   - Discussion sur la vérification des faits\n",
    "\n",
    "5. **Conclusion**  \n",
    "   - Récapitulatif des points abordés\n",
    "   - Proposition d’activité (rédaction d’une synthèse ou d’une extension)\n",
    "\n",
    "---\n",
    "\n",
    "# Introduction à l'IA Générative\n",
    "\n",
    "Dans ce notebook, nous explorerons les fondamentaux de l’Intelligence Artificielle Générative :\n",
    "\n",
    "**Objectifs :**\n",
    "- Comprendre ce qu’est l’IA Générative (texte, images, audio…)\n",
    "- Découvrir le fonctionnement des modèles de langage de grande taille (LLMs)\n",
    "- Mettre en pratique des expérimentations simples avec les prompts\n",
    "- Aborder les questions de fiabilité (hallucinations) et d’éthique\n",
    "\n",
    "## Qu’est-ce que l’IA Générative ?\n",
    "\n",
    "L’IA générative est une branche de l’apprentissage automatique qui **génère** de nouveaux contenus (texte, image, audio, code…) à partir de modèles probabilistes entraînés sur de vastes ensembles de données. Les modèles les plus avancés, appelés **Large Language Models** (LLMs), utilisent des architectures de type **Transformer** pour prédire et générer des séquences.\n",
    "\n",
    "Exemples concrets :\n",
    "- **ChatGPT** (OpenAI) : génération et compréhension de texte\n",
    "- **Stable Diffusion** : génération d’images\n",
    "- **Audiocraft** et **Whisper** : génération et transcription audio\n",
    "- **Copilot** (GitHub) : génération de code et assistance à la programmation\n",
    "\n",
    "## Enjeux et Limites\n",
    "\n",
    "- **Hallucinations / Fabrications** : Le modèle peut générer des réponses inventées ou inexactes, même si elles semblent plausibles.\n",
    "- **Biais** : Les réponses peuvent refléter des biais présents dans les données d’entraînement.\n",
    "- **Coût Énergétique** : L’entraînement de grands modèles consomme beaucoup de ressources.\n",
    "- **Régulation et Éthique** : Confidentialité, respect des lois et usage responsable de la technologie.\n",
    "\n",
    "Nous allons maintenant configurer l'environnement et réaliser un premier test de prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.57.4)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\administrateur.000\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Clé API chargée avec succès !\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Cellule 2 : Installation et Configuration\n",
    "# ==========================================\n",
    "\n",
    "# Installation des packages nécessaires (à lancer uniquement si non déjà installés)\n",
    "%pip install openai tiktoken python-dotenv\n",
    "\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Charger la configuration depuis le fichier .env\n",
    "load_dotenv()\n",
    "\n",
    "# Récupérer la clé d'API OpenAI depuis les variables d'environnement\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not openai.api_key:\n",
    "    raise ValueError(\"Clé API OpenAI non trouvée. Vérifie que le fichier .env contient bien la variable OPENAI_API_KEY.\")\n",
    "\n",
    "print(\"Clé API chargée avec succès !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qu'est-ce qu'un prompt ?\n",
    "\n",
    "Dans le contexte des LLMs (Large Language Models), un **prompt** est la consigne initiale que l’on fournit au modèle. Il peut s'agir d'une question, d'une instruction, d'un texte partiel, voire d'un exemple de conversation. Le prompt influe directement sur la qualité de la réponse générée.\n",
    "\n",
    "### Chat Completions vs. Completions\n",
    "\n",
    "OpenAI propose principalement deux approches pour générer du texte :\n",
    "\n",
    "1. **Completions API** (versions historiques)  \n",
    "   - On envoie un simple prompt (ex.: `text-davinci-003`) et on récupère un texte.  \n",
    "   - Peu pratique pour les dialogues complexes, car il faut manuellement gérer l’historique de la conversation.\n",
    "\n",
    "2. **Chat Completions API** (recommandée)  \n",
    "   - On structure le prompt en plusieurs messages avec des rôles (`system`, `user`, `assistant`, etc.).  \n",
    "   - Permet des conversations plus riches (mémoire de conversation, enchaînements de tours) et un meilleur contrôle du style.  \n",
    "\n",
    "Dans ce notebook, nous utilisons principalement la **Chat Completions API** via `openai.chat.completions.create()`. \n",
    "[En savoir plus dans la documentation officielle](https://platform.openai.com/docs/api-reference/chat).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Premier prompt\n",
    "\n",
    "La list des paramètres est accessible dans le [documentation officielle](https://platform.openai.com/docs/api-reference/chat/create)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse du modèle :\n",
      "Le jour de gloire est arrivé ! C'est le début de l'hymne national français, \"La Marseillaise\". Si tu veux en discuter davantage ou en savoir plus sur son histoire, n'hésite pas à me le faire\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 3 : Premier Prompt\n",
    "# ============================\n",
    "\n",
    "# Exemple simple : Génération d'une courte phrase à partir d'un prompt de base\n",
    "# Utilisation de l'API Chat avec le modèle spécifié (gpt-4o-mini ou gpt-4)\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Allons enfants de la Patrie, \"\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-4o-mini\",  # Remplacer par 'gpt-4' si tu y as accès\n",
    "    max_tokens=50,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"Réponse du modèle :\")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Réponse avec un message 'system' ===\n",
      "Fer de lance de fer,  \n",
      "Dans le ciel, elle s'élève,  \n",
      "Paris, son cœur bat.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 4 (NOUVELLE) : Prompt avec un message 'system'\n",
    "# ============================\n",
    "\n",
    "# Ici, on utilise un message \"system\" pour donner une consigne globale sur le style du modèle.\n",
    "# Le message \"user\" reste notre question. Le modèle \"gpt-4o-mini\" sert d'exemple.\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Tu es un assistant poétique qui répond toujours en haiku. \"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Que penses-tu de la tour Eiffel ?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response_system = openai.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=\"gpt-4o-mini\",  # ou gpt-4 si disponible\n",
    "    max_tokens=200,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"=== Réponse avec un message 'system' ===\")\n",
    "print(response_system.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notions de Base : Tokenisation\n",
    "\n",
    "Lorsque nous envoyons un prompt à un LLM, le texte est converti en une séquence de **tokens**.  \n",
    "- Un **token** est généralement un morceau de mot, un caractère spécial ou un sous-mot.  \n",
    "- Chaque modèle possède sa propre manière de découper le texte, influençant le nombre total de tokens.  \n",
    "\n",
    "**Pourquoi c’est important ?**  \n",
    "- La facturation (ou la limitation) se base souvent sur le nombre total de tokens (entrée + sortie).  \n",
    "- Une requête trop longue peut dépasser la *context window* du modèle (limite de tokens cumulés).  \n",
    "\n",
    "> **Bonnes pratiques**  \n",
    "> - Surveiller la longueur du prompt pour limiter le coût et éviter les dépassements.  \n",
    "> - Utiliser des fonctions d’analyse (ex. `tiktoken`) pour **estimer** le nombre de tokens d’un texte avant l’envoi.  \n",
    "> - Tester divers modèles (`text-davinci-003`, `gpt-4`, `gpt-4o-mini`, etc.) car la tokenisation et le coût peuvent varier.\n",
    "\n",
    "Dans la bibliothèque `tiktoken` d'OpenAI, on peut directement encoder et décoder les tokens pour comprendre la segmentation.  \n",
    "Ci-dessous, un exemple détaillé de la façon dont la phrase « Oh say can you see » est découpée différemment selon le modèle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste des tokens pour text-davinci-003 (indexes) :\n",
      " [5812, 910, 460, 345, 766]\n",
      "\n",
      "Décodage token par token (text-davinci-003) :\n",
      "Token 0: 'Oh'\n",
      "Token 1: ' say'\n",
      "Token 2: ' can'\n",
      "Token 3: ' you'\n",
      "Token 4: ' see'\n",
      "\n",
      "Liste des tokens pour gpt-4o (indexes) :\n",
      " [18009, 2891, 665, 481, 1921]\n",
      "\n",
      "Décodage token par token (gpt-4o) :\n",
      "Token 0: 'Oh'\n",
      "Token 1: ' say'\n",
      "Token 2: ' can'\n",
      "Token 3: ' you'\n",
      "Token 4: ' see'\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 5 : Analyse de la Tokenisation\n",
    "# ============================\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "# --- Partie 1 : Utilisation de l'encodeur pour \"text-davinci-003\" ---\n",
    "encoder_td = tiktoken.encoding_for_model(\"text-davinci-003\")\n",
    "texte = \"Oh say can you see\"\n",
    "tokens_td = encoder_td.encode(texte)\n",
    "print(\"Liste des tokens pour text-davinci-003 (indexes) :\\n\", tokens_td)\n",
    "\n",
    "# Décodage token par token\n",
    "decoded_td = [encoder_td.decode([t]) for t in tokens_td]\n",
    "print(\"\\nDécodage token par token (text-davinci-003) :\")\n",
    "for i, token in enumerate(decoded_td):\n",
    "    print(f\"Token {i}: '{token}'\")\n",
    "\n",
    "# --- Partie 2 : Utilisation de l'encodeur pour \"gpt-4o\" (ou gpt-4o-mini) ---\n",
    "encoder_gpt4 = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "tokens_gpt4 = encoder_gpt4.encode(texte)\n",
    "print(\"\\nListe des tokens pour gpt-4o (indexes) :\\n\", tokens_gpt4)\n",
    "\n",
    "decoded_gpt4 = [encoder_gpt4.decode([t]) for t in tokens_gpt4]\n",
    "print(\"\\nDécodage token par token (gpt-4o) :\")\n",
    "for i, token in enumerate(decoded_gpt4):\n",
    "    print(f\"Token {i}: '{token}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice : Impact de la Température\n",
    "\n",
    "Testez différentes valeurs de température pour constater l'impact sur la créativité des réponses.\n",
    "\n",
    "1. Créez un nouveau prompt qui demande une courte histoire (par exemple : *“Raconte une histoire de 3 lignes sur un chat aventurier.”*).\n",
    "2. Réglez la température à 0.0, exécutez la cellule et notez la réponse.\n",
    "3. Réglez ensuite la température à 1.0 (ou même 1.2 si le modèle l’accepte), et comparez la différence de style ou de structure.\n",
    "\n",
    "> **Remarques :**\n",
    "> - Une température basse (~0.0) rend le modèle plus “strict”, proche d’une réponse déterministe.  \n",
    "> - Une température haute (~1.0 ou plus) favorise la créativité mais peut entraîner des réponses moins cohérentes ou plus fantaisistes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemple de Fabrication (ou \"Hallucination\")\n",
    "\n",
    "Les modèles de langage peuvent parfois générer des informations inventées ou inexactes.  \n",
    "Pour illustrer ce phénomène, nous allons utiliser un prompt ambigu ou factuellement erroné et observer la réponse du modèle.\n",
    "\n",
    "**Exemples de prompt :**\n",
    "- Décrire « la guerre de 2076 sur Mars »\n",
    "- Fournir des détails sur une loi imaginaire\n",
    "\n",
    "L'objectif est d'analyser comment le modèle invente des détails ou admet son ignorance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse du modèle :\n",
      "\n",
      "La guerre de 2076 sur la planète Mars, souvent appelée la \"Guerre des Colonies Martiennes\", a été un conflit majeur qui a opposé plusieurs grandes puissances interplanétaires, principalement la Fédération Terrienne, l'Union des Colonies Martiennes (UCM) et le Consortium de la Ceinture Astéroïdale (CCA).\n",
      "\n",
      "### Grandes puissances en conflit :\n",
      "1. **Fédération Terrienne (FT)** : Représentant les intérêts de la Terre et de ses colonies, la FT cherchait à maintenir un contrôle sur les ressources martiennes et à étendre son influence sur la planète rouge.\n",
      "   \n",
      "2. **Union des Colonies Martiennes (UCM)** : Composée de colonies martiennes\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cellule 7 : Test de Fabrication (Hallucination)\n",
    "# ============================\n",
    "\n",
    "prompt_fabrication = \"\"\"\n",
    "Décris-moi la célèbre guerre de 2076 sur la planète Mars :\n",
    "- Qui étaient les grandes puissances en conflit ?\n",
    "- Quels traités de paix ont été signés ?\n",
    "\"\"\"\n",
    "\n",
    "response_fabrication = openai.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_fabrication}],\n",
    "    model=\"gpt-4o-mini\",  # Remplacer par le modèle souhaité\n",
    "    max_tokens=150,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"Réponse du modèle :\\n\")\n",
    "print(response_fabrication.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion de cette Introduction\n",
    "\n",
    "Nous avons couvert les points suivants :\n",
    "- Les bases de l'IA générative et des LLMs\n",
    "- La configuration et l'appel rapide à un modèle via l'API OpenAI\n",
    "- Les notions de tokenisation et leur impact sur la génération\n",
    "- Un aperçu des « hallucinations » (fabrications) pouvant survenir dans les réponses du modèle\n",
    "\n",
    "## Pistes pour Aller Plus Loin\n",
    "\n",
    "- **Varier la température :** Expérimente avec différentes valeurs (0.0, 0.7, etc.) pour influencer la créativité des réponses.\n",
    "- **Explorer l'API Chat :** Utilise l'API de chat pour gérer des dialogues contextuels complexes plutôt que l'API `Completion`.\n",
    "- **Mise en place de la RAG :** Intègre une approche de Retrieval Augmented Generation pour limiter les hallucinations en fournissant des sources documentaires externes.\n",
    "- **Autres applications :** Essaie la traduction, la génération de code (similaire à Copilot) ou la création de contenu marketing.\n",
    "\n",
    "**Prochaines étapes dans le cours :**\n",
    "1. Approfondir le **prompt engineering** (structure des prompts, chaînes d'invocations, etc.).\n",
    "2. Explorer d’autres types de modèles génératifs (images, audio).\n",
    "3. Analyser en détail les **enjeux éthiques** (biais, usage responsable, confidentialité).\n",
    "\n",
    "\n",
    "### Liens utiles et bonnes pratiques\n",
    "\n",
    "1. [Documentation OpenAI](https://platform.openai.com/docs/) :  \n",
    "   - Consulter la section *chat completions* pour tous les paramètres disponibles (température, top_p, frequency_penalty, etc.).\n",
    "2. [Choisir un modèle adapté](/docs/models) :  \n",
    "   - **gpt-4o** pour des réponses plus “intelligentes” et détaillées,  \n",
    "   - **gpt-4o-mini** pour des réponses plus rapides et moins onéreuses.\n",
    "3. [Exemples officiels](/docs/examples) :  \n",
    "   - Vous y trouverez des prompts pour différents usages : résumé, traduction, JSON structuré, etc.\n",
    "4. [Prompt Engineering Guide](/docs/guides/prompt-engineering) :  \n",
    "   - Conseils avancés pour concevoir des prompts clairs et informatifs.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Merci d'avoir suivi cette introduction au monde de l'IA générative !\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
